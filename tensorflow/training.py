""" 
The script can:

    - log the experiment on comet.ml
    - create a config file locally with the configuration in it
    - create a csv file with the val_loss and train_loss locally
    - save the checkpoints locally
"""
import sys
from os import mkdir, listdir, environ
from os.path import join, dirname, isfile, expanduser, basename, split
from shutil import copyfile
import argparse
import string
import random
import csv

from operator import itemgetter

from comet_ml import Experiment

import tensorflow

parser = argparse.ArgumentParser()
parser.add_argument('-c', '--configuration', help="Path to the directory containing the config file to use. The configuration file should be named 'config_file.py' (see the examples in the config folder of the repository).")
parser.add_argument('-r', '--restart', help="Restart the training from a previous stopped config. The argument is the path to the experiment folder.", type=str)
parser.add_argument('--comet', dest='comet', action='store_true', help="If the experiment should be saved to comet ml in addition to locally")
parser.add_argument('--no-comet', dest='comet', action='store_false')
parser.set_defaults(feature=False)
args = parser.parse_args()

# If we restart an experiment, no need to check for a configuration, we load the one from the config file.
if args.restart is not None:
    sys.path.append(join(args.restart, "config"))
    from saved_config import TrainingConfiguration
    key = dirname(args.restart).split("_")[-1]
    if args.restart.rfind("/") == len(args.restart) - 1:
        config = TrainingConfiguration(split(dirname(args.restart))[0], key)
    else:
        config = TrainingConfiguration(split(args.restart)[0], key)

    # We extract the last saved weight and the corresponding epoch
    checkpoint_path = join(args.restart, "checkpoints")

    # Getting the restart epoch
    config.weights = tensorflow.train.latest_checkpoint(checkpoint_path)
    restart_epoch = int(basename(config.weights).split("_")[0].split("-")[1])

    # Set the output dir
    output_dir = config.output_dir

else:
    sys.path.append(args.configuration)
    from config_file import TrainingConfiguration
    

    # Starting the experiment
    if args.comet:
        experiment = Experiment(api_key=environ["COMET_API_KEY"],
                                project_name=config.project_name, workspace=config.workspace)
        key = experiment.get_key()

    else:
        key = ''.join(random.choice(string.ascii_uppercase +
                                        string.ascii_lowercase + string.digits) for _ in range(32))
    
    config = TrainingConfiguration(environ["EXPERIMENTS_OUTPUT_DIRECTORY"], key)
    output_dir = config.output_dir

print("The experiment will be written to {}".format(output_dir))

checkpoints_output_dir = join(output_dir, "checkpoints")
config_output_dir = join(output_dir, "config")
results_output_dir = join(output_dir, "results")

if args.restart is None:
    # We create all the output directories
    mkdir(output_dir)
    mkdir(checkpoints_output_dir)
    mkdir(config_output_dir)
    mkdir(results_output_dir)

    copyfile(join(args.configuration, "config_file.py"), join(config_output_dir, "saved_config.py"))
    copyfile(join(args.configuration, "config_file.py"), join(config_output_dir, "temp_config.py"))

# Logging the experiment
if args.restart is None:
    if args.comet:
        experiment.log_parameters(config.__dict__)

# Creating the model
model = config.network

if args.restart:
    print("Loading weights (by name): {}".format(config.weights))
    model.load_weights(config.weights)
else:
    if config.weights is not None:
        print("Loading weights (by name): {}".format(config.weights))
        model.load_weights(config.weights, by_name=True)

# Prepare the generators & callbacks
config.prepare_training_generators()

# Compiling the model
model.compile(loss=config.loss,
                optimizer=config.optimizer,
                metrics=config.metrics)

if args.restart is not None:
    model.fit_generator(config.train_generator,
                        validation_data=config.validation_generator,
                        epochs=config.epochs,
                        steps_per_epoch=config.steps_per_epoch,
                        callbacks=config.callbacks,
                        workers=config.workers,
                        use_multiprocessing=config.multiprocessing,
                        initial_epoch=restart_epoch)
else:
    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(config.train_generator,
                            validation_data=config.validation_generator,
                            epochs=config.epochs,
                            steps_per_epoch=config.steps_per_epoch,
                            callbacks=config.callbacks,
                            workers=config.workers,
                            use_multiprocessing=config.multiprocessing)
